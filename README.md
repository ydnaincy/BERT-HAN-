# BERTâ€‘HAN++

![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)
![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)
![Model](https://img.shields.io/badge/Model-BERT--HAN%2B%2B-purple.svg)
![Platform](https://img.shields.io/badge/Platform-Cross--lingual%20NLP-orange.svg)
![Conference](https://img.shields.io/badge/Conference-Scopus--Indexed%20ICACCIS--2025-red.svg)

> **BERTâ€‘HAN++: A Crossâ€‘Lingual Hierarchical Transformer with Adaptive Complexity and SHAPâ€‘Attention Fusion for Efficient & Interpretable Document Classification**

---

## ğŸ§  Abstract

**BERTâ€‘HAN++** is a hybrid architecture that fuses:

* **BERT** for deep contextual encoding
* **Hierarchical Attention Networks (HAN)** for documentâ€‘level structure awareness
* **SHAPâ€‘enhanced interpretability** for faithful tokenâ€‘level rationales

### Key Contributions

| Innovation                      | Impact                                                     |
| ------------------------------- | ---------------------------------------------------------- |
| Adaptive Complexity Controller  | â¬‡ï¸ LatencyÂ &Â FLOPs by dynamically skipping inactive layers |
| Crossâ€‘Lingual Selfâ€‘Distillation | ğŸŒ Robust multilingual representations viaÂ *mBERT*         |
| Attnâ€‘SHAP Fusion                | âœ… Faithful, humanâ€‘readable explanations                    |
| Edgeâ€‘Friendly Quantisation      | âš¡ 8â€‘bit inferenceÂ (â‰¤0.3â€¯%â€¯Î”Acc) & 2.1Ã— speedâ€‘up            |

---

## ğŸ“Š Benchmarks

| Dataset              | Macroâ€‘F1Â (â†‘) | InferenceÂ SpeedÂ (â†“) | Notes                          |
| -------------------- | ------------ | ------------------- | ------------------------------ |
| AGÂ News              | **97.8**     | 1.0Ã—                | English, 4â€‘class               |
| DBPedia              | **99.3**     | 1.1Ã—                | English, 14â€‘class              |
| Yahoo Answers        | **80.4**     | 1.0Ã—                | English, 10â€‘class              |
| 20Â NG                | **89.7**     | 1.2Ã—                | English, 20â€‘class              |
| IMDB                 | **94.1**     | 1.0Ã—                | English, sentiment             |
| HindiÂ AGÂ News        | **95.6**     | 1.0Ã—                | âœ¨ Crossâ€‘lingual generalisation |
| SpanishÂ BillionÂ Word | **91.2**     | 1.0Ã—                | âœ¨ Crossâ€‘lingual generalisation |

*Numbers are averaged over 5 runs; speed reported relative to BERTâ€‘base.*

---

## ğŸ› ï¸ Core Modules

```
BERT Sentence Encoder   â”€â”
                         â”œâ”€â–¶ Hierarchical Document Transformer â”€â”
Adaptive Complexity Gates â”˜                                     â”‚
Crossâ€‘Lingual KD (mBERT)                                        â”‚
                                                               â–¼
                     Attnâ€‘SHAP Fusion  â”€â”€â–¶ INT8 Quantised Head â”€â”€â–¶ Output
```

---

## ğŸ“‚ Repository Layout

```bash
.
â”œâ”€â”€ models/                  # PyTorch / HF model definitions (BERTâ€‘HAN++)
â”œâ”€â”€ data/                    # ScriptsÂ + links for all datasets
â”œâ”€â”€ notebooks/               # Endâ€‘toâ€‘end Colab & Jupyter demos
â”œâ”€â”€ utils/                   # Helper utilities (logging, SHAP wrappers)
â”œâ”€â”€ results/                 # Saved weights, logs, figures & tables
â”œâ”€â”€ requirements.txt         # Python deps (PyTorchÂ >=1.13, transformersÂ >=4.40)
â”œâ”€â”€ README.md                # âœ¨ You are here
â””â”€â”€ bert_han_paper.pdf       # Cameraâ€‘ready ICACCISâ€‘2025 paper (optional)
```

---

## ğŸš€ QuickÂ Start

### 1.Â Clone & Install

```bash
git clone https://github.com/<YOURâ€‘ORG>/bertâ€‘hanâ€‘plusâ€‘plus.git
cd bertâ€‘hanâ€‘plusâ€‘plus
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

### 2.Â Train on a Benchmark

```bash
python train.py \
  --dataset ag_news \
  --epochs 5 \
  --adaptive-gating on \
  --int8-ptq off
```

### 3.Â Evaluate a Checkpoint

```bash
python evaluate.py \
  --dataset ag_news \
  --checkpoint results/ag_news/best.pt \
  --int8-ptq on
```

### 4.Â Visualise Explanations

```bash
python explain.py \
  --text "OpenAI acquires leading robotics startup." \
  --checkpoint results/ag_news/best.pt
```

A HTML heatâ€‘map will open in your browser showing Attnâ€‘SHAP token importances.

---


---

## ğŸ¤ Contributing

Pull requests are welcome! Please open an issue first to discuss any major changes.

1. Fork the repo
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a PR

---

## ğŸ›¡ï¸ License

This project is released under the MIT License â€” see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgements

* Hugging Face ğŸ¤— transformers & datasets
* SHAP Library
* PyTorch Lightning
s
